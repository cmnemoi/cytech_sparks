{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf23284",
   "metadata": {},
   "source": [
    "# Création d'un ELT avec Spark SQL\n",
    "\n",
    "Un ELT (Extract Load Transform) est un processus de transformation de données qui consiste à extraire des données d'une source, à les charger dans un entrepôt de données, puis à les transformer en un format adapté à l'analyse.\n",
    "\n",
    "Nous allons voir comment créer un ELT avec Spark SQL pour créer un jeu de données à partir de fichiers contenant les passagers du Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1552d1",
   "metadata": {},
   "source": [
    "## Configuration du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cefa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Hello, World from Scala kernel!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77739d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c89b80",
   "metadata": {},
   "source": [
    "## Extract\n",
    "\n",
    "Cette partie consiste à fusionner les données de plusieurs fichiers en un seul jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56005464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extract: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n",
       "load: (titanic: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "transform: (titanic: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "typeVariables: (titanic: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "translateToEnglish: (titanic: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract(spark: SparkSession): DataFrame = {\n",
    "    // load titanic files as DataFrames\n",
    "    val titanicPart1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/titanic_part_1.txt\")\n",
    "    val titanicPart2 = spark.read.format(\"parquet\").load(\"data/titanic_part_2.parquet\")\n",
    "    val titanicPart3 = spark.read.format(\"orc\").load(\"data/titanic_part_3.orc\")\n",
    "\n",
    "    // combine DataFrames\n",
    "    val titanic = titanicPart1.union(titanicPart2).union(titanicPart3).dropDuplicates()\n",
    "    \n",
    "    titanic\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979dbae",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "Cette partie consiste à charger les données fusionnées dans un fichier CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(titanic: DataFrame): DataFrame = {\n",
    "    // write dataframe to csv\n",
    "    titanic.write.format(\"csv\").option(\"header\", \"true\").save(\"data/titanic.csv\")\n",
    "    titanic\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cebb0",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc562d41",
   "metadata": {},
   "source": [
    "Cette partie consiste à transformer les données pour les rendre exploitables.\n",
    "\n",
    "- Typage des variables\n",
    "- Traduction en anglais\n",
    "- Ajout de nouvelles variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47726979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNewVariables(titanic: DataFrame): DataFrame = {\n",
    "    titanic\n",
    "        .withColumn(\"FamilySize\", col(\"SibSp\") + col(\"Parch\") + 1)\n",
    "        .withColumn(\"Title\", regexp_extract(col(\"Name\"), \"(\\\\w+\\\\.)\", 1))\n",
    "        .withColumn(\"AgeCategory\", when(col(\"Age\") < 20, \"Young\")\n",
    "            .when(col(\"Age\") < 40, \"Adult\")\n",
    "            .when(col(\"Age\") < 60, \"Old\")\n",
    "            .otherwise(\"Very Old\"))\n",
    "}\n",
    "\n",
    "def translateToEnglish(titanic: DataFrame): DataFrame = {\n",
    "    titanic\n",
    "        // replace all 'Monsieur' by 'Mr', and all 'Madame' by 'Mrs' in the \"Name\" column\n",
    "        .withColumn(\"Name\", regexp_replace(col(\"Name\"), \"Monsieur\", \"Mr\"))\n",
    "        .withColumn(\"Name\", regexp_replace(col(\"Name\"), \"Mamade\", \"Mrs\"))\n",
    "        // replace all \"femme\" by \"female\", and all \"homme\" by \"male in the \"Sex\" column\n",
    "        .withColumn(\"Sex\", regexp_replace(col(\"Sex\"), \"homme\", \"male\"))\n",
    "        .withColumn(\"Sex\", regexp_replace(col(\"Sex\"), \"femme\", \"female\"))\n",
    "}\n",
    "\n",
    "def typeVariables(titanic: DataFrame): DataFrame = {\n",
    "    titanic.select(titanic.columns.map {\n",
    "        case column@(\"PassengerId\" | \"Survived\" | \"Pclass\" | \"SibSp\" | \"Parch\") => titanic(column).cast(\"int\").as(column)\n",
    "        case column@(\"Age\" | \"Fare\") => titanic(column).cast(\"double\").as(column)\n",
    "        case column => titanic(column).cast(\"string\").as(column)\n",
    "    }:_*)\n",
    "}\n",
    "\n",
    "def transform(titanic: DataFrame): DataFrame = {\n",
    "    addNewVariables(translateToEnglish(typeVariables(titanic)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02e58b",
   "metadata": {},
   "source": [
    "## Exécution de l'ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae986ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val transformedTitanic = transform(load(extract(spark)))\n",
    "\n",
    "transformedTitanic.write.format(\"csv\").option(\"header\", \"true\").save(\"data/transformed_titanic.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
