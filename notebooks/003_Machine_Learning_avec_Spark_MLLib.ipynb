{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning avec Spark MLLib\n",
    "\n",
    "Dans ce notebook, nous allons utiliser Spark MLLib afin de prédire les survivants du Titanic. Pour cela, nous allons appliquer les étapes d'un projet de machine learning :\n",
    "\n",
    "- Définition du problème (classification, régression, clustering, ...)\n",
    "- Création d'un jeu de données d'entraînement et de test\n",
    "- Préparation des données\n",
    "- Création de modèles\n",
    "- Evaluation des modèles\n",
    "- Interprétation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://23d3897efa1d:4040\n",
       "SparkContext available as 'sc' (version = 3.5.0, master = local[*], app id = local-1704148130984)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World from Scala kernel!"
     ]
    }
   ],
   "source": [
    "// notebook setup\n",
    "print(\"Hello, World from Scala kernel!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition du problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit Xi les variables explicatives présentes dans le jeu de données Titanic : \n",
    " - Pclass : la classe du passager\n",
    " - Sex : le sexe du passager\n",
    " - Age : l'âge du passager\n",
    " - SibSp : le nombre de frères et soeurs / époux / épouses à bord\n",
    " - Parch : le nombre de parents / enfants à bord\n",
    " - Fare : le prix du ticket\n",
    " - Cabin : le numéro de cabine\n",
    " - Embarked : le port d'embarquement\n",
    "\n",
    "(nous avons retiré Name, Ticket, et PassengerId, car elles identifient de manière unique les passagers et ne sont pas donc pertinentes pour la prédiction)\n",
    "\n",
    "Soit Y la variable à prédire : Survived.\n",
    "\n",
    "Y étant une variable catégorielle (binaire), nous avons affaire à un problème de classification (binaire)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un jeu de données d'entraînement et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, chargeons le jeu de données Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titanic: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 13 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val titanic = spark\n",
    "                .read\n",
    "                .format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .load(\"../data/transformed_titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_d288b268358e\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lrModel = lr.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
