{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning avec Spark MLLib\n",
    "\n",
    "Dans ce notebook, nous allons faire une démonstration de Spark MLLib sur des données d'exemple. Pour cela, nous allons appliquer les étapes d'un projet de machine learning :\n",
    "\n",
    "- Création d'un jeu de données d'entraînement et de test\n",
    "- Création de modèles\n",
    "- Optimisation des hyperparamètres des modèles\n",
    "- Evaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://3c99054b149a:4040\n",
       "SparkContext available as 'sc' (version = 3.5.0, master = local[*], app id = local-1704568349298)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World from Scala kernel!"
     ]
    }
   ],
   "source": [
    "// notebook setup\n",
    "print(\"Hello, World from Scala kernel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un jeu de données d'entraînement et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, chargeons le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.format(\"libsvm\").load(\"../spark/data/mllib/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais créer un jeu d'entraînement et de test, en divisant le jeu de données aléatoirement en deux parties.\n",
    "\n",
    "Le jeu d'entraînement contiendra 80% des données, et le jeu de test 20%.\n",
    "\n",
    "Le jeu d'entraînement sera utilisé pour entraîner le modèle, et le jeu de test pour l'évaluer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSet = data.sample(false, 0.8)\n",
    "val testSet = data.except(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données représentant un problème de classification, nous allons utiliser des modèles de classification :\n",
    "- la régression logistique\n",
    "- les arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.tuning._\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification._;\n",
    "import org.apache.spark.ml.evaluation._;\n",
    "import org.apache.spark.ml.tuning._;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer des modèles, et les entraîner sur le jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decisionTree: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel: uid=dtc_7848116ac7f1, depth=2, numNodes=5, numClasses=2, numFeatures=692\n",
       "logisticRegression: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_392d9925285b, numClasses=2, numFeatures=692\n",
       "randomForest: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel: uid=rfc_2a16572cb413, numTrees=20, numClasses=2, numFeatures=692\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val decisionTree = new DecisionTreeClassifier().fit(trainingSet)\n",
    "val logisticRegression = new LogisticRegression().fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation des hyperparamètres du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une façon d'améliorer les performances d'un modèle est d'optimiser ses hyperparamètres à l'aide d'une recherche par grille.\n",
    "\n",
    "Pour rappel, les hyperparamètres sont des paramètres du modèle qui ne sont pas appris lors de l'entraînement, mais qui sont fixés avant l'entraînement. Par exemple, le nombre d'arbres dans une forêt aléatoire est un hyperparamètre.\n",
    "\n",
    "La recherche par grille consiste quant à elle à tester un ensemble de combinaisons d'hyperparamètres possibles, et à sélectionner celle qui donne les meilleurs résultats. Nous pouvons utiliser l'objet `ParamGridBuilder` de Spark pour cela.\n",
    "\n",
    "L'optimisation des hyperparamètres est généralement effectuées sur un jeu de validation. Nous pouvons utiliser l'objet `TrainValidationSplit` de Spark pour cela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des hyperparamètres possibles de la régression logistique sont :\n",
    "- `maxIter` : le nombre d'itérations\n",
    "- `regParam` : le paramètre de régularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logisticRegressionParameterGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_61b7158a6c73-maxIter: 10,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 100,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 1000,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 10,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 100,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 1000,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 10,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 100,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_61b7158a6c73-maxIter: 1000,\n",
       "\tlogreg_61b7158a6c73-regParam: 0.001\n",
       "})\n",
       "logisticRegressionTrainValidat...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logisticRegressionParameterGrid = new ParamGridBuilder()\n",
    "  .addGrid(logisticRegression.maxIter, Array(10, 100, 1000))\n",
    "  .addGrid(logisticRegression.regParam, Array(0.1, 0.01, 0.001))\n",
    "  .build() \n",
    "\n",
    "val logisticRegressionTrainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(new LogisticRegression())\n",
    "  .setEvaluator(new MulticlassClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(logisticRegressionParameterGrid)\n",
    "  .setTrainRatio(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logisticRegression: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_efceeaaa5e65, bestModel=LogisticRegressionModel: uid=logreg_5b455abad2a0, numClasses=2, numFeatures=692, trainRatio=0.8\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logisticRegression = logisticRegressionTrainValidationSplit.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des hyperparamètres possibles des arbres de décision sont :\n",
    "- `impurity` : la mesure d'impureté (indice de Gini ou l'entropie)\n",
    "- `maxDepth` : la profondeur maximale de l'arbre\n",
    "- `minInstancesPerNode` : le nombre minimal d'instances par noeud dasn l'arbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decisionTreeParameterGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tdtc_7848116ac7f1-impurity: gini,\n",
       "\tdtc_7848116ac7f1-maxDepth: 1,\n",
       "\tdtc_7848116ac7f1-minInstancesPerNode: 1\n",
       "}, {\n",
       "\tdtc_7848116ac7f1-impurity: entropy,\n",
       "\tdtc_7848116ac7f1-maxDepth: 1,\n",
       "\tdtc_7848116ac7f1-minInstancesPerNode: 1\n",
       "}, {\n",
       "\tdtc_7848116ac7f1-impurity: gini,\n",
       "\tdtc_7848116ac7f1-maxDepth: 2,\n",
       "\tdtc_7848116ac7f1-minInstancesPerNode: 1\n",
       "}, {\n",
       "\tdtc_7848116ac7f1-impurity: entropy,\n",
       "\tdtc_7848116ac7f1-maxDepth: 2,\n",
       "\tdtc_7848116ac7f1-minInstancesPerNode: 1\n",
       "}, {\n",
       "\tdtc_7848116ac7f1-impurity: gini,\n",
       "\tdtc_7848116ac7f1-maxDepth: 3,\n",
       "\tdtc_7848116ac7f1-minInstancesPerNode: 1\n",
       "}, {\n",
       "\tdtc_7848116ac7f1-impurity: entropy,\n",
       "\tdtc_7848116ac7f1-maxDepth: 3,\n",
       "\tdtc_7848116ac7f1-minInstancesPerNode: 1\n",
       "}, {\n",
       "\tdtc_7848116ac7f1-impurity: gini,\n",
       "\tdtc_7848...\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val decisionTreeParameterGrid = new ParamGridBuilder()\n",
    "  .addGrid(decisionTree.maxDepth, Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))\n",
    "  .addGrid(decisionTree.minInstancesPerNode, Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "  .addGrid(decisionTree.impurity, Array(\"gini\", \"entropy\"))\n",
    "  .build()\n",
    "\n",
    "val decisionTreeTrainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(new DecisionTreeClassifier())\n",
    "  .setEvaluator(new MulticlassClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(decisionTreeParameterGrid)\n",
    "  .setTrainRatio(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decisionTree: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_943a0b55ffaa, bestModel=DecisionTreeClassificationModel: uid=dtc_74c79fb7f64e, depth=2, numNodes=5, numClasses=2, numFeatures=692, trainRatio=0.8\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val decisionTree = decisionTreeTrainValidationSplit.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des modèles\n",
    "\n",
    "Nous pouvons désormais évaluer les modèles sur le jeu de test.\n",
    "\n",
    "Pour cela, nous allons utiliser la métrique d'exactitude (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 1.0\n",
      "Decision Tree Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logisticRegressionPredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n",
       "decisionTreePredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n",
       "logisticRegressionAccuracy: Double = 1.0\n",
       "decisionTreeAccuracy: Double = 1.0\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logisticRegressionPredictions = logisticRegression.transform(testSet)\n",
    "val decisionTreePredictions = decisionTree.transform(testSet)\n",
    "\n",
    "val logisticRegressionAccuracy = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "  .evaluate(logisticRegressionPredictions)\n",
    "\n",
    "val decisionTreeAccuracy = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")\n",
    "    .evaluate(decisionTreePredictions)\n",
    "\n",
    "println(\"Logistic Regression Accuracy: \" + logisticRegressionAccuracy)\n",
    "println(\"Decision Tree Accuracy: \" + decisionTreeAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression logistique et l'arbre de décision ont une accuracy de 100% sur le jeu de test : ils prédisent correctement la classe de toutes les instances du jeu de test !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
